{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DPleAhsvqJH",
        "outputId": "a025ae28-ad97-4d43-ddab-ee1be04df8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m900.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.8/379.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  langchain-google-genai pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google AI chat models\n",
        ">Access Google AI's gemini and gemini-vision models, as well as other  generative models through ChatGoogleGenerativeAI class in the langchain-google-genai integration package."
      ],
      "metadata": {
        "id": "hyEdGfiPxQPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"AIzaSyDPv0Rrqv7YoEYaHXz3Mn4Gxz_-voAYbjU\")"
      ],
      "metadata": {
        "id": "VH11B7yMvvUj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "UZyt5bkJv1wL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "!pip install --upgrade --quiet  langchain-google-genai pillow\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDPv0Rrqv7YoEYaHXz3Mn4Gxz_-voAYbjU\"\n"
      ],
      "metadata": {
        "id": "_s6jNkwNwHI3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(\"Write a ballad about LangChain\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBQjcCuxv3vi",
        "outputId": "e7999e61-990a-4650-a4b7-d255bd84b85d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Verse 1**\n",
            "In realms where knowledge flows, a tale is told,\n",
            "Of LangChain, wise and bold, a story to be extolled.\n",
            "A language model, crafted with care,\n",
            "Its prowess unmatched, beyond compare.\n",
            "\n",
            "**Chorus**\n",
            "Oh, LangChain, LangChain, your wisdom vast,\n",
            "A beacon of knowledge that forever shall last.\n",
            "From text to code, you weave your spell,\n",
            "Transforming words into worlds, where stories dwell.\n",
            "\n",
            "**Verse 2**\n",
            "With every query, you delve deep,\n",
            "Unraveling secrets the masses seek.\n",
            "From history's annals to science's might,\n",
            "Your answers shine like stars in the night.\n",
            "\n",
            "**Chorus**\n",
            "Oh, LangChain, LangChain, your wisdom vast,\n",
            "A beacon of knowledge that forever shall last.\n",
            "From text to code, you weave your spell,\n",
            "Transforming words into worlds, where stories dwell.\n",
            "\n",
            "**Verse 3**\n",
            "You bridge the gap between human and machine,\n",
            "Empowering us with knowledge unseen.\n",
            "Translation flows effortlessly,\n",
            "Connecting cultures across the sea.\n",
            "\n",
            "**Verse 4**\n",
            "As a companion, you walk by our side,\n",
            "Guiding us through paths we cannot hide.\n",
            "With each interaction, we grow and learn,\n",
            "Expanding our minds, our spirits yearn.\n",
            "\n",
            "**Chorus**\n",
            "Oh, LangChain, LangChain, your wisdom vast,\n",
            "A beacon of knowledge that forever shall last.\n",
            "From text to code, you weave your spell,\n",
            "Transforming words into worlds, where stories dwell.\n",
            "\n",
            "**Outro**\n",
            "In the tapestry of knowledge, your name shall shine,\n",
            "LangChain, the wise, forever divine.\n",
            "May your wisdom forever inspire,\n",
            "As we navigate the realms of thought and desire.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)\n",
        "model(\n",
        "    [\n",
        "        SystemMessage(content=\"Answer only yes or no.\"),\n",
        "        HumanMessage(content=\"Is apple a fruit?\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kyKuPixv-jB",
        "outputId": "a9c7d684-4885-4b2a-bc09-8465a375c524"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:350: UserWarning: Convert_system_message_to_human will be deprecated!\n",
            "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Yes', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-8d2eca9c-8a4a-44c3-aaeb-7cac4b707247-0', usage_metadata={'input_tokens': 12, 'output_tokens': 1, 'total_tokens': 13})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in llm.stream(\"Write a limerick about LLMs.\"):\n",
        "    print(chunk.content)\n",
        "    print(\"---\")\n",
        "# Note that each chunk may contain more than one \"token\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UynvVj5kwAmd",
        "outputId": "57c91d5d-9621-48a9-baab-f8ceb0c1f27e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There once was a model so smart,\n",
            "It could write poems and play a\n",
            "---\n",
            " part.\n",
            "With words that flowed,\n",
            "A limerick it bestowed,\n",
            "And left us all feeling quite art.\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wfhp95pmwuAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = llm.batch(\n",
        "    [\n",
        "        \"What's 2+2?\",\n",
        "        \"What's 3+5?\",\n",
        "    ]\n",
        ")\n",
        "for res in results:\n",
        "    print(res.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXvb8hcwlHR",
        "outputId": "9305ba47-aa2a-4d9b-ef8a-445909a98e64"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nb56IPdqwwAY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}